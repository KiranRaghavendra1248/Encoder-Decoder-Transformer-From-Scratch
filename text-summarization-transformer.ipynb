{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":32267,"sourceType":"datasetVersion","datasetId":24984},{"sourceId":791838,"sourceType":"datasetVersion","datasetId":1895}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install torch torchvision","metadata":{"_uuid":"2a4dffd8-ae9f-46bf-b4f9-a34e0cf9c73a","_cell_guid":"3c9d4f45-e1f5-4293-a2b0-bca5be6c1e96","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:57:11.654679Z","iopub.execute_input":"2024-02-23T10:57:11.655057Z","iopub.status.idle":"2024-02-23T10:57:25.808861Z","shell.execute_reply.started":"2024-02-23T10:57:11.655017Z","shell.execute_reply":"2024-02-23T10:57:25.807724Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.15.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.24.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nfrom spacy.tokenizer import Tokenizer\nfrom sklearn.model_selection import train_test_split\nimport spacy\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nfrom nltk.corpus import stopwords \nimport random\nfrom tqdm import tqdm\nimport math","metadata":{"_uuid":"700b5c85-a22a-47ad-8947-8134630ac7ac","_cell_guid":"2d247599-aee8-4a80-aac1-91428e1abbd4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:57:25.810880Z","iopub.execute_input":"2024-02-23T10:57:25.811228Z","iopub.status.idle":"2024-02-23T10:57:34.166846Z","shell.execute_reply.started":"2024-02-23T10:57:25.811201Z","shell.execute_reply":"2024-02-23T10:57:34.165827Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenizer using spacy\nnlp = spacy.load(\"en_core_web_sm\")\ntokenizer = Tokenizer(nlp.vocab)","metadata":{"_uuid":"b95b47d9-0858-4411-a24e-8c61e0fc5f2f","_cell_guid":"067027eb-375c-4c6f-b9fc-5cd9c4cb3b32","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:57:34.168015Z","iopub.execute_input":"2024-02-23T10:57:34.168495Z","iopub.status.idle":"2024-02-23T10:57:35.682922Z","shell.execute_reply.started":"2024-02-23T10:57:34.168470Z","shell.execute_reply":"2024-02-23T10:57:35.681831Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Add data from files into dataframe for easier access\ndef create_dataframe(source_text_path,target_text_path):\n    txt_files_source = [file for file in os.listdir(source_text_path) if file.endswith('.txt')]\n    txt_files_target = [file for file in os.listdir(target_text_path) if file.endswith('.txt')]\n    df = pd.DataFrame(columns=['headlines','text'])\n    for source,target in zip(txt_files_source,txt_files_target):\n        assert source==target\n        source_file_path = os.path.join(source_text_path, source)\n        target_file_path = os.path.join(target_text_path, target)\n        # Read the content of the file\n        with open(source_file_path,'r',encoding='latin-1') as file:\n            source_text = file.read()\n        with open(target_file_path,'r',encoding='latin-1') as file:\n            target_text = file.read()\n        df.loc[len(df.index)] = [source_text,target_text]\n    return df","metadata":{"_uuid":"b92ab9f0-5866-46a8-b849-ae8719c92f5b","_cell_guid":"f82ec7e3-93a8-4255-94e4-63e2c40bd974","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:57:35.685898Z","iopub.execute_input":"2024-02-23T10:57:35.686808Z","iopub.status.idle":"2024-02-23T10:57:35.695597Z","shell.execute_reply.started":"2024-02-23T10:57:35.686776Z","shell.execute_reply":"2024-02-23T10:57:35.694414Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Check accuracy function\ndef check_accuracy(output,labels):\n    _ , predpos = output.max(1)\n    num_samples=len(labels)\n    num_correct=(predpos==labels).sum()\n    return (num_correct/num_samples)*100\n\n# Save checkpoint\ndef save_checkpoint(state,filename='weights.pth.tar'):\n    print('Saving weights-->')\n    torch.save(state,filename)\n\n# Load checkpoint\ndef load_checkpoint(checkpoint,model,optim):\n    print('Loading weights-->')\n    model.load_state_dict(checkpoint['state_dict'])\n    optim.load_state_dict(checkpoint['optimizer'])","metadata":{"_uuid":"8b6f5c51-e82f-4cd5-a9cc-a61a11ad37f5","_cell_guid":"b2893126-8744-4671-b717-f4cf8bdb4c2b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:57:35.697055Z","iopub.execute_input":"2024-02-23T10:57:35.697417Z","iopub.status.idle":"2024-02-23T10:57:35.707115Z","shell.execute_reply.started":"2024-02-23T10:57:35.697383Z","shell.execute_reply":"2024-02-23T10:57:35.706303Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df1 = create_dataframe(\"/kaggle/input/bbc-news-summary/BBC News Summary/News Articles/business\",\"/kaggle/input/bbc-news-summary/BBC News Summary/Summaries/business\")\ndf2 = create_dataframe(\"/kaggle/input/bbc-news-summary/BBC News Summary/News Articles/entertainment\",\"/kaggle/input/bbc-news-summary/BBC News Summary/Summaries/entertainment\")\ndf3 = create_dataframe(\"/kaggle/input/bbc-news-summary/BBC News Summary/News Articles/politics\",\"/kaggle/input/bbc-news-summary/BBC News Summary/Summaries/politics\")\ndf4 = create_dataframe(\"/kaggle/input/bbc-news-summary/BBC News Summary/News Articles/sport\",\"/kaggle/input/bbc-news-summary/BBC News Summary/Summaries/sport\")\ndf5 = create_dataframe(\"/kaggle/input/bbc-news-summary/BBC News Summary/News Articles/tech\",\"/kaggle/input/bbc-news-summary/BBC News Summary/Summaries/tech\")","metadata":{"_uuid":"46f6b582-9775-46c2-96b6-86b3090e02a8","_cell_guid":"bc809679-bab7-4df3-a1ff-393f439c69ba","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:57:35.708453Z","iopub.execute_input":"2024-02-23T10:57:35.708828Z","iopub.status.idle":"2024-02-23T10:57:59.229205Z","shell.execute_reply.started":"2024-02-23T10:57:35.708794Z","shell.execute_reply":"2024-02-23T10:57:59.228214Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)","metadata":{"_uuid":"097b4032-8148-4928-90c5-93458bf8722a","_cell_guid":"a7721268-6a09-4ed5-bd6a-fbe26f4431a2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:57:59.230490Z","iopub.execute_input":"2024-02-23T10:57:59.230801Z","iopub.status.idle":"2024-02-23T10:57:59.236456Z","shell.execute_reply.started":"2024-02-23T10:57:59.230777Z","shell.execute_reply":"2024-02-23T10:57:59.235520Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Split into train and test sets\ndf = df.rename(columns = {\"headlines\":\"source_text\",\"text\":\"summary_text\"})\nX,Y = df[\"source_text\"],df[\"summary_text\"]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\ntrain_df = pd.DataFrame({'source_text': X_train, 'summary_text': Y_train})\ntest_df = pd.DataFrame({'source_text': X_test, 'summary_text': Y_test})","metadata":{"_uuid":"be69aa76-4e10-485c-993a-2916f7e049fc","_cell_guid":"5006b05a-9dd6-4057-9029-93da701eae37","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:57:59.237811Z","iopub.execute_input":"2024-02-23T10:57:59.238167Z","iopub.status.idle":"2024-02-23T10:57:59.260694Z","shell.execute_reply.started":"2024-02-23T10:57:59.238140Z","shell.execute_reply":"2024-02-23T10:57:59.259605Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\n                           \"you're\": \"you are\", \"you've\": \"you have\"}\n\n\nstop_words = set(stopwords.words('english'))","metadata":{"_uuid":"6483dce4-6451-45f0-abe2-920d83ffe252","_cell_guid":"1dcb9b05-c8bd-440b-aaff-f2d18e303e26","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:57:59.262111Z","iopub.execute_input":"2024-02-23T10:57:59.262446Z","iopub.status.idle":"2024-02-23T10:57:59.284116Z","shell.execute_reply.started":"2024-02-23T10:57:59.262421Z","shell.execute_reply":"2024-02-23T10:57:59.283240Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def text_cleaner(text):\n    newString = text.lower()\n    newString = newString.replace('\"', \"'\")\n    newString = re.sub(r'\\([^)]*\\)', '', newString)\n    newString = re.sub('\"','', newString)\n    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n    newString = re.sub(r\"'s\\b\",\"\",newString)\n    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n    tokens = [w for w in newString.split() if not w in stop_words]\n    return \" \".join(tokens)","metadata":{"_uuid":"42d338b4-69d5-4f5a-9a96-6104b23f276f","_cell_guid":"99fa5612-c457-4ffe-b7f9-812d3a5b6a99","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:57:59.289115Z","iopub.execute_input":"2024-02-23T10:57:59.289466Z","iopub.status.idle":"2024-02-23T10:57:59.297288Z","shell.execute_reply.started":"2024-02-23T10:57:59.289436Z","shell.execute_reply":"2024-02-23T10:57:59.296417Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Tokenize and lowercase text using spacy\ntrain_df['source_text'] = train_df['source_text'].apply(lambda x: [token.text.lower() for token in tokenizer(text_cleaner(x))])\ntrain_df['summary_text'] = train_df['summary_text'].apply(lambda x: [token.text.lower() for token in tokenizer(text_cleaner(x))])\n\ntest_df['source_text'] = test_df['source_text'].apply(lambda x: [token.text.lower() for token in tokenizer(text_cleaner(x))])\ntest_df['summary_text'] = test_df['summary_text'].apply(lambda x: [token.text.lower() for token in tokenizer(text_cleaner(x))])","metadata":{"_uuid":"c9c39fa0-fd53-4662-9113-e318e0f0e6c9","_cell_guid":"c6473982-99f3-40ce-be76-3f80bf0089a8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:57:59.298553Z","iopub.execute_input":"2024-02-23T10:57:59.298877Z","iopub.status.idle":"2024-02-23T10:58:03.739039Z","shell.execute_reply.started":"2024-02-23T10:57:59.298851Z","shell.execute_reply":"2024-02-23T10:58:03.737994Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Add START AND END tokens to summary\ntrain_df['source_text'] = train_df['source_text'].apply(lambda x : ['_START_']+ x + ['_END_'])\ntrain_df['summary_text'] = train_df['summary_text'].apply(lambda x : ['_START_']+ x + ['_END_'])\n\ntest_df['source_text'] = test_df['source_text'].apply(lambda x : ['_START_']+ x + ['_END_'])\ntest_df['summary_text'] = test_df['summary_text'].apply(lambda x : ['_START_']+ x + ['_END_'])","metadata":{"_uuid":"1abca708-aaf8-48f8-87ed-92cadf8fcecb","_cell_guid":"9f56dd7d-bd32-464f-9ea7-ff58173993a2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:58:03.740434Z","iopub.execute_input":"2024-02-23T10:58:03.740811Z","iopub.status.idle":"2024-02-23T10:58:03.782857Z","shell.execute_reply.started":"2024-02-23T10:58:03.740776Z","shell.execute_reply":"2024-02-23T10:58:03.781846Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"_uuid":"35758d71-f295-4443-ac0b-966d8529dd2c","_cell_guid":"e00823a4-42ba-49d4-ba30-25c124ab9900","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:58:03.783965Z","iopub.execute_input":"2024-02-23T10:58:03.784281Z","iopub.status.idle":"2024-02-23T10:58:03.810447Z","shell.execute_reply.started":"2024-02-23T10:58:03.784255Z","shell.execute_reply":"2024-02-23T10:58:03.809422Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                            source_text  \\\n1490  [_START_, ferguson, fears, milan, cutting, edg...   \n2001  [_START_, ask, jeeves, joins, web, log, market...   \n1572  [_START_, safin, cool, wimbledon, newly, crown...   \n1840  [_START_, mobiles, rack, years, use, mobile, p...   \n610   [_START_, eminem, secret, gig, venue, revealed...   \n\n                                           summary_text  \n1490  [_START_, loss, could, worse, quality, bring, ...  \n2001  [_START_, jim, lanzone, vice, president, searc...  \n1572  [_START_, expect, sampras, favourite, pressure...  \n1840  [_START_, cellnet, vodafone, mobile, phone, op...  \n610   [_START_, fourth, album, rap, star, sale, two,...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source_text</th>\n      <th>summary_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1490</th>\n      <td>[_START_, ferguson, fears, milan, cutting, edg...</td>\n      <td>[_START_, loss, could, worse, quality, bring, ...</td>\n    </tr>\n    <tr>\n      <th>2001</th>\n      <td>[_START_, ask, jeeves, joins, web, log, market...</td>\n      <td>[_START_, jim, lanzone, vice, president, searc...</td>\n    </tr>\n    <tr>\n      <th>1572</th>\n      <td>[_START_, safin, cool, wimbledon, newly, crown...</td>\n      <td>[_START_, expect, sampras, favourite, pressure...</td>\n    </tr>\n    <tr>\n      <th>1840</th>\n      <td>[_START_, mobiles, rack, years, use, mobile, p...</td>\n      <td>[_START_, cellnet, vodafone, mobile, phone, op...</td>\n    </tr>\n    <tr>\n      <th>610</th>\n      <td>[_START_, eminem, secret, gig, venue, revealed...</td>\n      <td>[_START_, fourth, album, rap, star, sale, two,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Build vocabularies - each word has an index, note : words sorted in ascending order\nall_tokens = train_df['source_text'].tolist() + train_df['summary_text'].tolist() + test_df['source_text'].tolist() + test_df['summary_text'].tolist()\nsource_vocab = {actual_word: idx for idx, (word_num, actual_word) in enumerate(sorted(enumerate(set(token for tokens in all_tokens for token in tokens)), key=lambda x: x[1]))}\ntarget_vocab = {actual_word: idx for idx, (word_num, actual_word) in enumerate(sorted(enumerate(set(token for tokens in all_tokens for token in tokens)), key=lambda x: x[1]))}","metadata":{"execution":{"iopub.status.busy":"2024-02-23T10:58:03.811632Z","iopub.execute_input":"2024-02-23T10:58:03.811935Z","iopub.status.idle":"2024-02-23T10:58:04.099698Z","shell.execute_reply.started":"2024-02-23T10:58:03.811909Z","shell.execute_reply":"2024-02-23T10:58:04.098619Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using\",device)","metadata":{"_uuid":"e93e303f-3d29-460d-a111-868ed90cc48b","_cell_guid":"8cf18b62-d506-4f7d-b6bf-3bd4ad72e696","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:58:04.100999Z","iopub.execute_input":"2024-02-23T10:58:04.101348Z","iopub.status.idle":"2024-02-23T10:58:04.107539Z","shell.execute_reply.started":"2024-02-23T10:58:04.101320Z","shell.execute_reply":"2024-02-23T10:58:04.106462Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Using cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"temp = list(sorted(source_vocab.items()))\nfor word, idx in temp[-5:]:\n    print(word,idx)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T10:58:04.108918Z","iopub.execute_input":"2024-02-23T10:58:04.109274Z","iopub.status.idle":"2024-02-23T10:58:04.132741Z","shell.execute_reply.started":"2024-02-23T10:58:04.109246Z","shell.execute_reply":"2024-02-23T10:58:04.131715Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"zuluaga 27632\nzurich 27633\nzutons 27634\nzvonareva 27635\nzvyagintsev 27636\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define a custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, source_texts, target_summaries, source_vocab, target_vocab):\n        self.source_texts = source_texts\n        self.target_summaries = target_summaries\n        self.source_vocab = source_vocab\n        self.target_vocab = target_vocab\n\n    def __len__(self):\n        return len(self.source_texts)\n\n    def __getitem__(self, idx):\n        source_text = [self.source_vocab[word] for word in self.source_texts[idx]]\n        target_summary = [self.target_vocab[word] for word in self.target_summaries[idx]]\n        return torch.tensor(source_text), torch.tensor(target_summary)","metadata":{"_uuid":"6d7d5a8f-a1ad-4c2f-a21f-3e82823667ed","_cell_guid":"46bd33d9-801b-46ed-96ec-2615e7c6fadf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:58:04.134071Z","iopub.execute_input":"2024-02-23T10:58:04.134395Z","iopub.status.idle":"2024-02-23T10:58:04.144124Z","shell.execute_reply.started":"2024-02-23T10:58:04.134368Z","shell.execute_reply":"2024-02-23T10:58:04.143083Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Create custom datasets\ntrain_dataset = CustomDataset(train_df['source_text'].tolist(), train_df['summary_text'].tolist(),source_vocab, target_vocab)\ntest_dataset = CustomDataset(test_df['source_text'].tolist(), test_df['summary_text'].tolist(),source_vocab, target_vocab)","metadata":{"_uuid":"d9b69a91-2f5e-4d36-a732-b6973369af63","_cell_guid":"a54a744c-1146-4fbf-906e-bc6ca22c2e2e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:58:04.145378Z","iopub.execute_input":"2024-02-23T10:58:04.145669Z","iopub.status.idle":"2024-02-23T10:58:04.163882Z","shell.execute_reply.started":"2024-02-23T10:58:04.145644Z","shell.execute_reply":"2024-02-23T10:58:04.162870Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def get_max_seqlen():\n    max_length = 0\n    for index, row in train_df.iterrows():\n        # Calculate the length of the current row\n        row_length = len(row['source_text'])\n        # Update the maximum length if the current row length is greater\n        max_length = max(max_length, row_length)\n    for index, row in test_df.iterrows():\n        # Calculate the length of the current row\n        row_length = len(row['source_text'])\n        # Update the maximum length if the current row length is greater\n        max_length = max(max_length, row_length)\n    print(\"Max length in dataset \",max_length)\n    return max_length","metadata":{"execution":{"iopub.status.busy":"2024-02-23T10:58:04.178946Z","iopub.execute_input":"2024-02-23T10:58:04.179279Z","iopub.status.idle":"2024-02-23T10:58:04.191344Z","shell.execute_reply.started":"2024-02-23T10:58:04.179254Z","shell.execute_reply":"2024-02-23T10:58:04.190384Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"'''\nNote : \nIn PyTorch, the `collate_fn` parameter in the `DataLoader` can be either a function or an object of a class. Both approaches are valid, and the choice depends on your preference and the complexity of your collation logic.\n\n1. Function as `collate_fn`:\ndef my_collate_fn(batch):\n    # Your custom collation logic here\n    return processed_batch\n# Use the function with DataLoader\ntrain_loader = DataLoader(dataset, batch_size=64, collate_fn=my_collate_fn)\n\n2. Class as `collate_fn`:\nclass MyCollateClass:\n    def __call__(self, batch):\n        # Your custom collation logic here\n        return processed_batch\n# Instantiate the class and use it with DataLoader\nmy_collate_instance = MyCollateClass()\ntrain_loader = DataLoader(dataset, batch_size=64, collate_fn=my_collate_instance)\n\nUsing a class allows you to maintain state between batches if needed, as the class instance retains its state between calls. This can be beneficial if your collation logic requires some persistent information.\n\nThe key point is that the `collate_fn` parameter should be a callable (a function or an object with a `__call__` method) that takes a list of batch data and returns the processed batch. The processing typically involves padding sequences, converting data types, or any other necessary steps to prepare the batch for the model.\n'''","metadata":{"_uuid":"18e145a3-ccf4-4682-8d42-5f9d454c9d7d","_cell_guid":"08a4bb2a-c30d-4ce9-8b18-c6fad1ece10e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:58:04.192555Z","iopub.execute_input":"2024-02-23T10:58:04.192860Z","iopub.status.idle":"2024-02-23T10:58:04.213781Z","shell.execute_reply.started":"2024-02-23T10:58:04.192835Z","shell.execute_reply":"2024-02-23T10:58:04.212834Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'\\nNote : \\nIn PyTorch, the `collate_fn` parameter in the `DataLoader` can be either a function or an object of a class. Both approaches are valid, and the choice depends on your preference and the complexity of your collation logic.\\n\\n1. Function as `collate_fn`:\\ndef my_collate_fn(batch):\\n    # Your custom collation logic here\\n    return processed_batch\\n# Use the function with DataLoader\\ntrain_loader = DataLoader(dataset, batch_size=64, collate_fn=my_collate_fn)\\n\\n2. Class as `collate_fn`:\\nclass MyCollateClass:\\n    def __call__(self, batch):\\n        # Your custom collation logic here\\n        return processed_batch\\n# Instantiate the class and use it with DataLoader\\nmy_collate_instance = MyCollateClass()\\ntrain_loader = DataLoader(dataset, batch_size=64, collate_fn=my_collate_instance)\\n\\nUsing a class allows you to maintain state between batches if needed, as the class instance retains its state between calls. This can be beneficial if your collation logic requires some persistent information.\\n\\nThe key point is that the `collate_fn` parameter should be a callable (a function or an object with a `__call__` method) that takes a list of batch data and returns the processed batch. The processing typically involves padding sequences, converting data types, or any other necessary steps to prepare the batch for the model.\\n'"},"metadata":{}}]},{"cell_type":"code","source":"# Define collate function for DataLoader\ndef collate_fn(batch):\n    sources, targets = zip(*batch)\n    padded_sources = pad_sequence(sources, batch_first=True)\n    padded_targets = pad_sequence(targets, batch_first=True)\n    return padded_sources, padded_targets","metadata":{"_uuid":"2d5d8275-0ac4-49be-b58c-653b0b3bafb5","_cell_guid":"043364db-8a6c-4b30-9690-870387393ff1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:58:04.214938Z","iopub.execute_input":"2024-02-23T10:58:04.215243Z","iopub.status.idle":"2024-02-23T10:58:04.230052Z","shell.execute_reply.started":"2024-02-23T10:58:04.215219Z","shell.execute_reply":"2024-02-23T10:58:04.229108Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, embedding_dim, num_heads):\n        super(MultiHeadAttention,self).__init__()\n        assert embedding_dim % num_heads == 0, \"embedding_dim must be divisible by num_heads\"\n\n        self.embedding_dim = embedding_dim\n        self.num_heads = num_heads\n        self.dim_perhead = embedding_dim // num_heads\n\n        self.W_q = nn.Linear(embedding_dim, embedding_dim)\n        self.W_k = nn.Linear(embedding_dim, embedding_dim)\n        self.W_v = nn.Linear(embedding_dim, embedding_dim)\n        self.W_o = nn.Linear(embedding_dim, embedding_dim)\n\n    def scaled_dot_product_attention(self,Q,K,V,mask=None):\n        # Q,K,V Shape : [Batch_Size X Num_Heads X Seq_len X Dim Per Head]\n        \n        K = K.transpose(-2,-1) # K = K.permute(0,1,3,2) also works\n        # K Shape(after permute) : [Batch_Size X Num_Heads X Dim Per Head X Seq_len]\n        attn_scores = torch.matmul(Q,K) / math.sqrt(self.dim_perhead)\n        # attn_scores Shape : [Batch_Size X Num_Heads X Seq_len X Seq_len]\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n        attn_probs = torch.softmax(attn_scores, dim=-1)\n        # attn_probs Shape : [Batch_Size X Num_Heads X Seq_len X Seq_len]\n        output = torch.matmul(attn_probs, V)\n        # output Shape : [Batch_Size X Num_Heads X Seq_len X Dim Per Head]\n        return output\n\n    def split_heads(self, x):\n        # X shape : [Batch_Size X Seq_len X Embedding Dim]\n        batch_size, seq_length, d_model = x.size()\n        x = x.view(batch_size, seq_length,self.num_heads,self.dim_perhead)\n        # X shape : [Batch_Size X Seq_len X Num_Heads X Dim Per Head]\n        x = x.transpose(1,2)\n        # X shape : [Batch_Size X Num_Heads X Seq_len X Dim Per Head]\n        return x\n\n    def combine_heads(self, x):\n        # x Shape : [Batch_Size X Num_Heads X Seq_len X Dim Per Head]\n        batch_size, _, seq_length, dim_perhead = x.size()\n        x = x.transpose(1,2).contiguous()\n        # x Shape : [Batch_Size X Seq_len X Num_Heads X Dim Per Head]\n        x = x.view(batch_size, seq_length,self.embedding_dim)\n        # x Shape : [Batch_Size X Seq_len X Embedding Dim]\n        return x\n\n    def forward(self, Q, K, V, mask=None):\n        # Q,K,V Shape : [Batch_Size X Seq_len X Embedding Dim]\n        Q = self.split_heads(self.W_q(Q)) \n        K = self.split_heads(self.W_k(K)) \n        V = self.split_heads(self.W_v(V)) \n        # Q,K,V Shape : [Batch_Size X Num_Heads X Seq_len X Dim Per Head]\n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        # attn_output Shape : [Batch_Size X Num_Heads X Seq_len X Dim Per Head]\n        output = self.W_o(self.combine_heads(attn_output))\n        # output Shape :  # x Shape : [Batch_Size X Seq_len X Embedding Dim]\n        return output","metadata":{"_uuid":"e701a107-dd0f-4b26-b071-56477bdea3e9","_cell_guid":"d4b8085d-1cf1-41bd-ae8a-43883decf3f3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:58:04.231464Z","iopub.execute_input":"2024-02-23T10:58:04.231867Z","iopub.status.idle":"2024-02-23T10:58:04.257574Z","shell.execute_reply.started":"2024-02-23T10:58:04.231831Z","shell.execute_reply":"2024-02-23T10:58:04.256531Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        # shape does not change here\n        return self.fc2(F.relu(self.fc1(x)))\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length):\n        super(PositionalEncoding, self).__init__()\n\n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length,dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model,2).float() * -(math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer('pe',pe.unsqueeze(0))\n\n    def forward(self, x):\n        # shape does not change here, adding positional encoding information\n        return x + self.pe[:, :x.size(1)]","metadata":{"execution":{"iopub.status.busy":"2024-02-23T10:58:04.258991Z","iopub.execute_input":"2024-02-23T10:58:04.259728Z","iopub.status.idle":"2024-02-23T10:58:04.279531Z","shell.execute_reply.started":"2024-02-23T10:58:04.259685Z","shell.execute_reply":"2024-02-23T10:58:04.278348Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        # x shape [Batch_Size X Seq_len X Embedding Dim]\n        attn_output = self.self_attn(x, x, x,mask)\n        # attn_output shape [Batch_Size X Seq_len X Embedding Dim]\n        x = self.norm1(x + self.dropout(attn_output))\n        # x shape [Batch_Size X Seq_len X Embedding Dim]\n        ff_output = self.feed_forward(x)\n        # ff_output shape [Batch_Size X Seq_len X Embedding Dim]\n        x = self.norm2(x + self.dropout(ff_output))\n        # x shape [Batch_Size X Seq_len X Embedding Dim]\n        return x\n\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(DecoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        attn_output = self.self_attn(x, x, x,tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        attn_output = self.cross_attn(x,enc_output,enc_output,src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-02-23T10:58:04.280991Z","iopub.execute_input":"2024-02-23T10:58:04.281406Z","iopub.status.idle":"2024-02-23T10:58:04.303590Z","shell.execute_reply.started":"2024-02-23T10:58:04.281374Z","shell.execute_reply":"2024-02-23T10:58:04.302520Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n        super(Transformer, self).__init__()\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n\n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n\n        self.fc = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def generate_mask(self, src, tgt):\n        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n        seq_length = tgt.size(1)\n        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length,device=device), diagonal=1)).bool()\n        tgt_mask = tgt_mask & nopeak_mask\n        return src_mask, tgt_mask\n\n    def forward(self, src, tgt):\n        src_mask, tgt_mask = self.generate_mask(src, tgt)\n        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n\n        enc_output = src_embedded\n        for enc_layer in self.encoder_layers:\n            enc_output = enc_layer(enc_output, src_mask)\n\n        dec_output = tgt_embedded\n        for dec_layer in self.decoder_layers:\n            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n\n        output = self.fc(dec_output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-02-23T10:58:09.858980Z","iopub.execute_input":"2024-02-23T10:58:09.859383Z","iopub.status.idle":"2024-02-23T10:58:09.872597Z","shell.execute_reply.started":"2024-02-23T10:58:09.859351Z","shell.execute_reply":"2024-02-23T10:58:09.871615Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"src_vocab_size = len(source_vocab)\ntgt_vocab_size = len(target_vocab)\nd_model = 512\nnum_heads = 8\nnum_layers = 6\nd_ff = 2048\nmax_seq_length = get_max_seqlen()\ndropout = 0.1\nnum_workers = 2\nnum_epochs = 10\n\nmodel = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\nprint(model)","metadata":{"_uuid":"7f7c4295-2263-4b6d-bc6b-f3292eca5f59","_cell_guid":"277eb600-7ff8-4537-9523-f6a0e8d7ac41","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:58:12.530851Z","iopub.execute_input":"2024-02-23T10:58:12.531555Z","iopub.status.idle":"2024-02-23T10:58:13.562244Z","shell.execute_reply.started":"2024-02-23T10:58:12.531522Z","shell.execute_reply":"2024-02-23T10:58:13.561226Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Max length in dataset  1986\nTransformer(\n  (encoder_embedding): Embedding(27637, 512)\n  (decoder_embedding): Embedding(27637, 512)\n  (positional_encoding): PositionalEncoding()\n  (encoder_layers): ModuleList(\n    (0-5): 6 x EncoderLayer(\n      (self_attn): MultiHeadAttention(\n        (W_q): Linear(in_features=512, out_features=512, bias=True)\n        (W_k): Linear(in_features=512, out_features=512, bias=True)\n        (W_v): Linear(in_features=512, out_features=512, bias=True)\n        (W_o): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (feed_forward): PositionWiseFeedForward(\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n      )\n      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (decoder_layers): ModuleList(\n    (0-5): 6 x DecoderLayer(\n      (self_attn): MultiHeadAttention(\n        (W_q): Linear(in_features=512, out_features=512, bias=True)\n        (W_k): Linear(in_features=512, out_features=512, bias=True)\n        (W_v): Linear(in_features=512, out_features=512, bias=True)\n        (W_o): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (cross_attn): MultiHeadAttention(\n        (W_q): Linear(in_features=512, out_features=512, bias=True)\n        (W_k): Linear(in_features=512, out_features=512, bias=True)\n        (W_v): Linear(in_features=512, out_features=512, bias=True)\n        (W_o): Linear(in_features=512, out_features=512, bias=True)\n      )\n      (feed_forward): PositionWiseFeedForward(\n        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n      )\n      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (fc): Linear(in_features=512, out_features=27637, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(trainable_params)","metadata":{"_uuid":"61cd290d-00e4-4bc7-849c-547a025553fc","_cell_guid":"35ea08c6-d8d5-4da4-b347-6d5085b93464","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:59:19.573234Z","iopub.execute_input":"2024-02-23T10:59:19.573629Z","iopub.status.idle":"2024-02-23T10:59:19.580870Z","shell.execute_reply.started":"2024-02-23T10:59:19.573597Z","shell.execute_reply":"2024-02-23T10:59:19.579686Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"86616565\n","output_type":"stream"}]},{"cell_type":"code","source":"# Specify optimizer and loss function\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)","metadata":{"_uuid":"e96fa9e8-c5be-4cc1-931d-57218cc5cc5a","_cell_guid":"8b1eb0a0-a1ec-4c88-909a-b9f03447e16f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:59:20.404987Z","iopub.execute_input":"2024-02-23T10:59:20.405925Z","iopub.status.idle":"2024-02-23T10:59:20.413536Z","shell.execute_reply.started":"2024-02-23T10:59:20.405887Z","shell.execute_reply":"2024-02-23T10:59:20.412428Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=num_workers)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)","metadata":{"_uuid":"feaf3b05-e23a-4335-a2e4-5747cfca4665","_cell_guid":"d2074be6-23b0-42d2-9a57-45d9f58f72fd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:59:30.931863Z","iopub.execute_input":"2024-02-23T10:59:30.932240Z","iopub.status.idle":"2024-02-23T10:59:30.937903Z","shell.execute_reply.started":"2024-02-23T10:59:30.932210Z","shell.execute_reply":"2024-02-23T10:59:30.936820Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"source_dummy,target_dummy = next(iter(train_loader))","metadata":{"_uuid":"e6d7609a-1211-4982-9045-5eb2acafe357","_cell_guid":"95b67a6a-93f3-49f6-9c39-9bd2bab10daf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:59:32.867627Z","iopub.execute_input":"2024-02-23T10:59:32.868054Z","iopub.status.idle":"2024-02-23T10:59:32.969822Z","shell.execute_reply.started":"2024-02-23T10:59:32.867999Z","shell.execute_reply":"2024-02-23T10:59:32.968468Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"print(source_dummy.shape,target_dummy.shape)","metadata":{"_uuid":"997ef9b3-127e-4454-9b89-3a6f861bc687","_cell_guid":"0533af63-7e19-4979-8cdc-c2369106ddc6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T10:59:36.629985Z","iopub.execute_input":"2024-02-23T10:59:36.630940Z","iopub.status.idle":"2024-02-23T10:59:36.636526Z","shell.execute_reply.started":"2024-02-23T10:59:36.630901Z","shell.execute_reply":"2024-02-23T10:59:36.635461Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"torch.Size([4, 167]) torch.Size([4, 71])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(source_dummy[1])","metadata":{"execution":{"iopub.status.busy":"2024-02-23T10:59:38.927218Z","iopub.execute_input":"2024-02-23T10:59:38.927586Z","iopub.status.idle":"2024-02-23T10:59:38.936046Z","shell.execute_reply.started":"2024-02-23T10:59:38.927558Z","shell.execute_reply":"2024-02-23T10:59:38.934908Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"tensor([    1, 19619, 27499,  1048, 10077,  3514, 17903, 19619, 10575,  8715,\n        24938,  6192, 26986,  4867, 27314,  5754,  5549,  4032, 27479, 17131,\n         4974,  8441, 23436, 15044,  9673,  5522, 26129, 18915, 14542, 15038,\n         1192, 19531, 17903, 27338, 11765,  1443, 10077, 24509, 21344, 27578,\n        11865, 18199, 25639,  1498, 27299, 26986,   151, 27316, 27201, 16221,\n         4937, 15038, 25249, 19619, 16326, 14881,  6200, 25503,  6121, 24509,\n        16792,  3118, 24509, 15499, 11141, 27488, 21344, 24509, 27338, 25794,\n        19619, 17252,  8441, 27338,  8875, 17903, 24509, 21344,  8397,  5754,\n         5549,  4028, 20345,  1498,  2042, 12315, 23221,  1089,   209, 24509,\n         2792, 15600, 26161,  1302, 25794, 19084, 19619, 27314,  5754,  5549,\n         4028, 15858, 13934, 27479,  8441, 12461,    78, 10077, 24509,  3173,\n         3226,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(torch.min(target_dummy),torch.max(target_dummy))","metadata":{"execution":{"iopub.status.busy":"2024-02-23T10:59:44.040794Z","iopub.execute_input":"2024-02-23T10:59:44.041208Z","iopub.status.idle":"2024-02-23T10:59:44.048901Z","shell.execute_reply.started":"2024-02-23T10:59:44.041177Z","shell.execute_reply":"2024-02-23T10:59:44.047870Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"tensor(0) tensor(27578)\n","output_type":"stream"}]},{"cell_type":"code","source":"model.to(device)\nsource_dummy = source_dummy.to(device)\ntarget_dummy = target_dummy.to(device)\nprint()","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:00:30.002098Z","iopub.execute_input":"2024-02-23T11:00:30.002513Z","iopub.status.idle":"2024-02-23T11:00:30.013579Z","shell.execute_reply.started":"2024-02-23T11:00:30.002481Z","shell.execute_reply":"2024-02-23T11:00:30.012584Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = model(source_dummy,target_dummy)\nprint(y_pred.shape,target_dummy.shape)","metadata":{"_uuid":"72e12e35-d2b8-4b26-bab0-bae78e8cb7ad","_cell_guid":"8a03d1e5-dca4-485b-9240-c4195be82280","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T11:00:30.985075Z","iopub.execute_input":"2024-02-23T11:00:30.985940Z","iopub.status.idle":"2024-02-23T11:00:31.317911Z","shell.execute_reply.started":"2024-02-23T11:00:30.985905Z","shell.execute_reply":"2024-02-23T11:00:31.316968Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"torch.Size([4, 71, 27637]) torch.Size([4, 71])\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = y_pred.reshape(-1,len(target_vocab))\ntarget_dummy = target_dummy.reshape(-1)\nprint(y_pred.shape,target_dummy.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:00:37.321372Z","iopub.execute_input":"2024-02-23T11:00:37.321802Z","iopub.status.idle":"2024-02-23T11:00:37.328394Z","shell.execute_reply.started":"2024-02-23T11:00:37.321751Z","shell.execute_reply":"2024-02-23T11:00:37.327349Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"torch.Size([284, 27637]) torch.Size([284])\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_loop(model,dataloader,loss_fun,optimizer,device):\n    model.train()\n    model.to(device)\n    min_loss = None\n    for epoch in range(num_epochs):\n        losses = []\n        accuracies = []\n        loop = tqdm(enumerate(dataloader), total=len(dataloader), leave=True)\n        for batch,(x,y) in loop:\n            # put on cuda\n            x = x.to(device)\n            y = y.to(device)\n    \n            # forward pass\n            y_pred = model(x,y)\n            \n            # calculate loss & accuracy\n            loss = loss_fun(y_pred.reshape(-1,len(target_vocab)),y.reshape(-1))\n            losses.append(loss.detach().item())\n            \n            accuracy = check_accuracy(y_pred.reshape(-1,len(target_vocab)),y.reshape(-1))\n            accuracies.append(accuracy.detach().item())\n            \n            # zero out prior gradients\n            optimizer.zero_grad()\n            \n            # backprop\n            loss.backward()\n            \n            # update weights\n            optimizer.step()\n            scheduler.step()\n            \n            # Update TQDM progress bar\n            loop.set_description(f\"Epoch [{epoch}/{num_epochs}] \")\n            loop.set_postfix(loss=loss.detach().item(), accuracy=accuracy.detach().item())\n\n        moving_loss = sum(losses) / len(losses)\n        moving_accuracy = sum(accuracies) / len(accuracies)\n        checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n        # Save check point\n        if min_loss == None:\n            min_loss = moving_loss\n            save_checkpoint(checkpoint)\n        elif moving_loss < min_loss:\n            min_loss = moving_loss\n            save_checkpoint(checkpoint)\n        print('Epoch {0} : Loss = {1} , Training Accuracy={2}'.format(epoch, moving_loss, moving_accuracy))","metadata":{"_uuid":"f1e3fed7-78bd-4512-b670-7542eab6abad","_cell_guid":"7400fc0b-dbe2-4935-9e5a-dcab47e23275","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T11:00:41.986927Z","iopub.execute_input":"2024-02-23T11:00:41.987318Z","iopub.status.idle":"2024-02-23T11:00:41.999142Z","shell.execute_reply.started":"2024-02-23T11:00:41.987290Z","shell.execute_reply":"2024-02-23T11:00:41.998195Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"train_loop(model,train_loader,criterion,optimizer,device)/","metadata":{"_uuid":"3d0cdde5-7cf9-4b33-befc-62b2f9da463b","_cell_guid":"d133f546-0ae6-416c-8b66-9527fe6bc73b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T11:00:43.749192Z","iopub.execute_input":"2024-02-23T11:00:43.749575Z","iopub.status.idle":"2024-02-23T11:16:51.435842Z","shell.execute_reply.started":"2024-02-23T11:00:43.749545Z","shell.execute_reply":"2024-02-23T11:16:51.434412Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"Epoch [0/20] : 100%|| 445/445 [00:46<00:00,  9.57it/s, accuracy=30.5, loss=5.09]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 0 : Loss = 6.908239778240075 , Training Accuracy=22.35531384620104\n","output_type":"stream"},{"name":"stderr","text":"Epoch [1/20] : 100%|| 445/445 [00:46<00:00,  9.66it/s, accuracy=52.6, loss=3.74]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 1 : Loss = 3.831281638949105 , Training Accuracy=45.11859307449855\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/20] : 100%|| 445/445 [00:45<00:00,  9.70it/s, accuracy=52.5, loss=2.21]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 2 : Loss = 2.6866609562648818 , Training Accuracy=52.25829454914907\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/20] : 100%|| 445/445 [00:45<00:00,  9.81it/s, accuracy=69.9, loss=2.17]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 3 : Loss = 2.0379160607798714 , Training Accuracy=57.111336024423665\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/20] : 100%|| 445/445 [00:46<00:00,  9.65it/s, accuracy=48.3, loss=1.7]  \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 4 : Loss = 1.6177765642659048 , Training Accuracy=58.71403843740399\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/20] : 100%|| 445/445 [00:45<00:00,  9.68it/s, accuracy=57.7, loss=1.35] \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 5 : Loss = 1.3235397495580523 , Training Accuracy=60.27891213492061\n","output_type":"stream"},{"name":"stderr","text":"Epoch [6/20] : 100%|| 445/445 [00:45<00:00,  9.70it/s, accuracy=56.9, loss=0.614]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 6 : Loss = 1.1019645950767432 , Training Accuracy=62.13116863122147\n","output_type":"stream"},{"name":"stderr","text":"Epoch [7/20] : 100%|| 445/445 [00:46<00:00,  9.67it/s, accuracy=70.2, loss=0.748]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 7 : Loss = 0.9267714869440271 , Training Accuracy=62.74302706772022\n","output_type":"stream"},{"name":"stderr","text":"Epoch [8/20] : 100%|| 445/445 [00:46<00:00,  9.63it/s, accuracy=76.6, loss=1.05] \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 8 : Loss = 0.784919796331545 , Training Accuracy=63.98580661302202\n","output_type":"stream"},{"name":"stderr","text":"Epoch [9/20] : 100%|| 445/445 [00:46<00:00,  9.66it/s, accuracy=60.5, loss=1.27] \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 9 : Loss = 0.6648119542036164 , Training Accuracy=64.68886709749029\n","output_type":"stream"},{"name":"stderr","text":"Epoch [10/20] : 100%|| 445/445 [00:46<00:00,  9.66it/s, accuracy=59.7, loss=0.258]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 10 : Loss = 0.5669622163759189 , Training Accuracy=64.90695223433248\n","output_type":"stream"},{"name":"stderr","text":"Epoch [11/20] : 100%|| 445/445 [00:45<00:00,  9.76it/s, accuracy=69.3, loss=0.437]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 11 : Loss = 0.47678658459963424 , Training Accuracy=66.56710979590255\n","output_type":"stream"},{"name":"stderr","text":"Epoch [12/20] : 100%|| 445/445 [00:45<00:00,  9.73it/s, accuracy=57.4, loss=0.574]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 12 : Loss = 0.40671895399187386 , Training Accuracy=67.30556015700436\n","output_type":"stream"},{"name":"stderr","text":"Epoch [13/20] : 100%|| 445/445 [00:45<00:00,  9.76it/s, accuracy=62.5, loss=0.472] \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 13 : Loss = 0.3395778112699476 , Training Accuracy=67.07324917396802\n","output_type":"stream"},{"name":"stderr","text":"Epoch [14/20] : 100%|| 445/445 [00:45<00:00,  9.75it/s, accuracy=84.7, loss=0.265] \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 14 : Loss = 0.2860099634720703 , Training Accuracy=67.17069450549864\n","output_type":"stream"},{"name":"stderr","text":"Epoch [15/20] : 100%|| 445/445 [00:46<00:00,  9.67it/s, accuracy=80.6, loss=0.176] \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 15 : Loss = 0.23685560856809776 , Training Accuracy=66.76500736193711\n","output_type":"stream"},{"name":"stderr","text":"Epoch [16/20] : 100%|| 445/445 [00:45<00:00,  9.79it/s, accuracy=84.9, loss=0.254] \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 16 : Loss = 0.19521482404363288 , Training Accuracy=67.18848783085855\n","output_type":"stream"},{"name":"stderr","text":"Epoch [17/20] : 100%|| 445/445 [00:45<00:00,  9.75it/s, accuracy=69.1, loss=0.249] \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 17 : Loss = 0.16056065750674586 , Training Accuracy=67.22025282356176\n","output_type":"stream"},{"name":"stderr","text":"Epoch [18/20] : 100%|| 445/445 [00:45<00:00,  9.76it/s, accuracy=63.3, loss=0.1]   \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 18 : Loss = 0.12928644109182477 , Training Accuracy=67.06444563276312\n","output_type":"stream"},{"name":"stderr","text":"Epoch [19/20] : 100%|| 445/445 [00:45<00:00,  9.68it/s, accuracy=86.1, loss=0.0905]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 19 : Loss = 0.10339200578713685 , Training Accuracy=67.06262954540468\n","output_type":"stream"}]},{"cell_type":"code","source":"def test_loop(model,dataloader,loss_fun,device):\n    model.eval()\n    model.to(device)\n    losses = []\n    samples,correct = 0,0\n    loop = tqdm(enumerate(dataloader), total=len(dataloader), leave=True)\n    with torch.no_grad():\n        for batch,(x,y) in loop:\n            # put on cuda\n            x = x.to(device)\n            y = y.to(device)\n\n            # forward pass\n            y_pred = model(x,y)\n            \n            # caclulate test loss\n            loss = loss_fun(y_pred.reshape(-1,len(target_vocab)),y.reshape(-1))\n            losses.append(loss.detach().item())\n\n            # accuracy over entire dataset\n            _,predpos=y_pred.reshape(-1,len(target_vocab)).max(1)\n            samples+=len(y.reshape(-1))\n            correct+=(predpos==y.reshape(-1)).sum().item()\n            \n            # Update TQDM progress bar\n            loop.set_postfix(loss=loss.item())\n\n    print(\"Final Test Accuracy = \",100 * (correct/samples))","metadata":{"_uuid":"aa0e9514-c139-42a4-baf2-c60e1a4ea2d4","_cell_guid":"79a91b46-bf45-436a-b90c-659ec99fc05a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T11:18:25.264645Z","iopub.execute_input":"2024-02-23T11:18:25.265037Z","iopub.status.idle":"2024-02-23T11:18:25.275663Z","shell.execute_reply.started":"2024-02-23T11:18:25.264993Z","shell.execute_reply":"2024-02-23T11:18:25.274605Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"test_loop(model,test_loader,criterion,device)","metadata":{"_uuid":"762d68a9-2cd0-47e9-a014-09d7e7c126a5","_cell_guid":"b1f7396c-b765-4e86-bf12-3255a83c84fe","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-23T11:18:25.979649Z","iopub.execute_input":"2024-02-23T11:18:25.980523Z","iopub.status.idle":"2024-02-23T11:18:29.762811Z","shell.execute_reply.started":"2024-02-23T11:18:25.980485Z","shell.execute_reply":"2024-02-23T11:18:29.761603Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"100%|| 112/112 [00:03<00:00, 30.23it/s, loss=0.22] ","output_type":"stream"},{"name":"stdout","text":"Final Test Accuracy =  60.11538962071808\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}