{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":32267,"sourceType":"datasetVersion","datasetId":24984},{"sourceId":791838,"sourceType":"datasetVersion","datasetId":1895}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install torch torchvision torchaudio","metadata":{"_uuid":"2a4dffd8-ae9f-46bf-b4f9-a34e0cf9c73a","_cell_guid":"3c9d4f45-e1f5-4293-a2b0-bca5be6c1e96","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:57:45.286766Z","iopub.execute_input":"2024-02-09T09:57:45.287028Z","iopub.status.idle":"2024-02-09T09:57:58.826569Z","shell.execute_reply.started":"2024-02-09T09:57:45.287003Z","shell.execute_reply":"2024-02-09T09:57:58.825364Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.15.1)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.24.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torchtext.vocab import GloVe\nfrom spacy.tokenizer import Tokenizer\nfrom sklearn.model_selection import train_test_split\nimport spacy\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nfrom nltk.corpus import stopwords \nimport random\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer, BertModel, BertConfig","metadata":{"_uuid":"700b5c85-a22a-47ad-8947-8134630ac7ac","_cell_guid":"2d247599-aee8-4a80-aac1-91428e1abbd4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:57:58.828763Z","iopub.execute_input":"2024-02-09T09:57:58.829094Z","iopub.status.idle":"2024-02-09T09:58:10.508320Z","shell.execute_reply.started":"2024-02-09T09:57:58.829061Z","shell.execute_reply":"2024-02-09T09:58:10.507518Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenizer using spacy\nnlp = spacy.load(\"en_core_web_sm\")\ntokenizer = Tokenizer(nlp.vocab)","metadata":{"_uuid":"b95b47d9-0858-4411-a24e-8c61e0fc5f2f","_cell_guid":"067027eb-375c-4c6f-b9fc-5cd9c4cb3b32","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:10.509383Z","iopub.execute_input":"2024-02-09T09:58:10.509911Z","iopub.status.idle":"2024-02-09T09:58:11.720078Z","shell.execute_reply.started":"2024-02-09T09:58:10.509884Z","shell.execute_reply":"2024-02-09T09:58:11.719254Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Add data from files into dataframe for easier access\ndef create_dataframe(source_text_path,target_text_path):\n    txt_files_source = [file for file in os.listdir(source_text_path) if file.endswith('.txt')]\n    txt_files_target = [file for file in os.listdir(target_text_path) if file.endswith('.txt')]\n    df = pd.DataFrame(columns=['headlines','text'])\n    for source,target in zip(txt_files_source,txt_files_target):\n        assert source==target\n        source_file_path = os.path.join(source_text_path, source)\n        target_file_path = os.path.join(target_text_path, target)\n        # Read the content of the file\n        with open(source_file_path,'r',encoding='latin-1') as file:\n            source_text = file.read()\n        with open(target_file_path,'r',encoding='latin-1') as file:\n            target_text = file.read()\n        df.loc[len(df.index)] = [source_text,target_text]\n    return df","metadata":{"_uuid":"b92ab9f0-5866-46a8-b849-ae8719c92f5b","_cell_guid":"f82ec7e3-93a8-4255-94e4-63e2c40bd974","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:11.722112Z","iopub.execute_input":"2024-02-09T09:58:11.722407Z","iopub.status.idle":"2024-02-09T09:58:11.729663Z","shell.execute_reply.started":"2024-02-09T09:58:11.722381Z","shell.execute_reply":"2024-02-09T09:58:11.728791Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Check accuracy function\ndef check_accuracy(output,labels):\n    _ , predpos = output.max(1)\n    num_samples=len(labels)\n    num_correct=(predpos==labels).sum()\n    return (num_correct/num_samples)*100\n\n# Save checkpoint\ndef save_checkpoint(state,filename='weights.pth.tar'):\n    print('Saving weights-->')\n    torch.save(state,filename)\n\n# Load checkpoint\ndef load_checkpoint(checkpoint,model,optim):\n    print('Loading weights-->')\n    model.load_state_dict(checkpoint['state_dict'])\n    optim.load_state_dict(checkpoint['optimizer'])","metadata":{"_uuid":"8b6f5c51-e82f-4cd5-a9cc-a61a11ad37f5","_cell_guid":"b2893126-8744-4671-b717-f4cf8bdb4c2b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:11.730719Z","iopub.execute_input":"2024-02-09T09:58:11.730982Z","iopub.status.idle":"2024-02-09T09:58:11.740767Z","shell.execute_reply.started":"2024-02-09T09:58:11.730959Z","shell.execute_reply":"2024-02-09T09:58:11.739882Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df1 = create_dataframe(\"/kaggle/input/bbc-news-summary/BBC News Summary/News Articles/business\",\"/kaggle/input/bbc-news-summary/BBC News Summary/Summaries/business\")\ndf2 = create_dataframe(\"/kaggle/input/bbc-news-summary/BBC News Summary/News Articles/entertainment\",\"/kaggle/input/bbc-news-summary/BBC News Summary/Summaries/entertainment\")\ndf3 = create_dataframe(\"/kaggle/input/bbc-news-summary/BBC News Summary/News Articles/politics\",\"/kaggle/input/bbc-news-summary/BBC News Summary/Summaries/politics\")\ndf4 = create_dataframe(\"/kaggle/input/bbc-news-summary/BBC News Summary/News Articles/sport\",\"/kaggle/input/bbc-news-summary/BBC News Summary/Summaries/sport\")\ndf5 = create_dataframe(\"/kaggle/input/bbc-news-summary/BBC News Summary/News Articles/tech\",\"/kaggle/input/bbc-news-summary/BBC News Summary/Summaries/tech\")","metadata":{"_uuid":"46f6b582-9775-46c2-96b6-86b3090e02a8","_cell_guid":"bc809679-bab7-4df3-a1ff-393f439c69ba","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:11.741906Z","iopub.execute_input":"2024-02-09T09:58:11.742175Z","iopub.status.idle":"2024-02-09T09:58:46.835657Z","shell.execute_reply.started":"2024-02-09T09:58:11.742151Z","shell.execute_reply":"2024-02-09T09:58:46.834769Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)","metadata":{"_uuid":"097b4032-8148-4928-90c5-93458bf8722a","_cell_guid":"a7721268-6a09-4ed5-bd6a-fbe26f4431a2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:46.837191Z","iopub.execute_input":"2024-02-09T09:58:46.837583Z","iopub.status.idle":"2024-02-09T09:58:46.843174Z","shell.execute_reply.started":"2024-02-09T09:58:46.837549Z","shell.execute_reply":"2024-02-09T09:58:46.842274Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Split into train and test sets\ndf = df.rename(columns = {\"headlines\":\"source_text\",\"text\":\"summary_text\"})\nX,Y = df[\"source_text\"],df[\"summary_text\"]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\ntrain_df = pd.DataFrame({'source_text': X_train, 'summary_text': Y_train})\ntest_df = pd.DataFrame({'source_text': X_test, 'summary_text': Y_test})","metadata":{"_uuid":"be69aa76-4e10-485c-993a-2916f7e049fc","_cell_guid":"5006b05a-9dd6-4057-9029-93da701eae37","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:46.844485Z","iopub.execute_input":"2024-02-09T09:58:46.844762Z","iopub.status.idle":"2024-02-09T09:58:46.867082Z","shell.execute_reply.started":"2024-02-09T09:58:46.844738Z","shell.execute_reply":"2024-02-09T09:58:46.866265Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\n                           \"you're\": \"you are\", \"you've\": \"you have\"}\n\n\nstop_words = set(stopwords.words('english'))","metadata":{"_uuid":"6483dce4-6451-45f0-abe2-920d83ffe252","_cell_guid":"1dcb9b05-c8bd-440b-aaff-f2d18e303e26","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:46.868779Z","iopub.execute_input":"2024-02-09T09:58:46.869044Z","iopub.status.idle":"2024-02-09T09:58:46.888795Z","shell.execute_reply.started":"2024-02-09T09:58:46.869021Z","shell.execute_reply":"2024-02-09T09:58:46.887816Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def text_cleaner(text):\n    newString = text.lower()\n    newString = newString.replace('\"', \"'\")\n    newString = re.sub(r'\\([^)]*\\)', '', newString)\n    newString = re.sub('\"','', newString)\n    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n    newString = re.sub(r\"'s\\b\",\"\",newString)\n    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n    tokens = [w for w in newString.split() if not w in stop_words]\n    return \" \".join(tokens)","metadata":{"_uuid":"42d338b4-69d5-4f5a-9a96-6104b23f276f","_cell_guid":"99fa5612-c457-4ffe-b7f9-812d3a5b6a99","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:46.893843Z","iopub.execute_input":"2024-02-09T09:58:46.894136Z","iopub.status.idle":"2024-02-09T09:58:46.900863Z","shell.execute_reply.started":"2024-02-09T09:58:46.894111Z","shell.execute_reply":"2024-02-09T09:58:46.899999Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Tokenize and lowercase text using spacy\ntrain_df['source_text'] = train_df['source_text'].apply(lambda x: [token.text.lower() for token in tokenizer(text_cleaner(x))])\ntrain_df['summary_text'] = train_df['summary_text'].apply(lambda x: [token.text.lower() for token in tokenizer(text_cleaner(x))])\n\ntest_df['source_text'] = test_df['source_text'].apply(lambda x: [token.text.lower() for token in tokenizer(text_cleaner(x))])\ntest_df['summary_text'] = test_df['summary_text'].apply(lambda x: [token.text.lower() for token in tokenizer(text_cleaner(x))])","metadata":{"_uuid":"c9c39fa0-fd53-4662-9113-e318e0f0e6c9","_cell_guid":"c6473982-99f3-40ce-be76-3f80bf0089a8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:46.902073Z","iopub.execute_input":"2024-02-09T09:58:46.902441Z","iopub.status.idle":"2024-02-09T09:58:51.051165Z","shell.execute_reply.started":"2024-02-09T09:58:46.902399Z","shell.execute_reply":"2024-02-09T09:58:51.050251Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Add START AND END tokens to summary\ntrain_df['source_text'] = train_df['source_text'].apply(lambda x : ['_START_']+ x + ['_END_'])\ntrain_df['summary_text'] = train_df['summary_text'].apply(lambda x : ['_START_']+ x + ['_END_'])\n\ntest_df['source_text'] = test_df['source_text'].apply(lambda x : ['_START_']+ x + ['_END_'])\ntest_df['summary_text'] = test_df['summary_text'].apply(lambda x : ['_START_']+ x + ['_END_'])","metadata":{"_uuid":"1abca708-aaf8-48f8-87ed-92cadf8fcecb","_cell_guid":"9f56dd7d-bd32-464f-9ea7-ff58173993a2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:51.052479Z","iopub.execute_input":"2024-02-09T09:58:51.053081Z","iopub.status.idle":"2024-02-09T09:58:51.091644Z","shell.execute_reply.started":"2024-02-09T09:58:51.053046Z","shell.execute_reply":"2024-02-09T09:58:51.090764Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"_uuid":"35758d71-f295-4443-ac0b-966d8529dd2c","_cell_guid":"e00823a4-42ba-49d4-ba30-25c124ab9900","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:51.092812Z","iopub.execute_input":"2024-02-09T09:58:51.093160Z","iopub.status.idle":"2024-02-09T09:58:51.119158Z","shell.execute_reply.started":"2024-02-09T09:58:51.093130Z","shell.execute_reply":"2024-02-09T09:58:51.118076Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                            source_text  \\\n1490  [_START_, ferguson, fears, milan, cutting, edg...   \n2001  [_START_, ask, jeeves, joins, web, log, market...   \n1572  [_START_, safin, cool, wimbledon, newly, crown...   \n1840  [_START_, mobiles, rack, years, use, mobile, p...   \n610   [_START_, eminem, secret, gig, venue, revealed...   \n\n                                           summary_text  \n1490  [_START_, loss, could, worse, quality, bring, ...  \n2001  [_START_, jim, lanzone, vice, president, searc...  \n1572  [_START_, expect, sampras, favourite, pressure...  \n1840  [_START_, cellnet, vodafone, mobile, phone, op...  \n610   [_START_, fourth, album, rap, star, sale, two,...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source_text</th>\n      <th>summary_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1490</th>\n      <td>[_START_, ferguson, fears, milan, cutting, edg...</td>\n      <td>[_START_, loss, could, worse, quality, bring, ...</td>\n    </tr>\n    <tr>\n      <th>2001</th>\n      <td>[_START_, ask, jeeves, joins, web, log, market...</td>\n      <td>[_START_, jim, lanzone, vice, president, searc...</td>\n    </tr>\n    <tr>\n      <th>1572</th>\n      <td>[_START_, safin, cool, wimbledon, newly, crown...</td>\n      <td>[_START_, expect, sampras, favourite, pressure...</td>\n    </tr>\n    <tr>\n      <th>1840</th>\n      <td>[_START_, mobiles, rack, years, use, mobile, p...</td>\n      <td>[_START_, cellnet, vodafone, mobile, phone, op...</td>\n    </tr>\n    <tr>\n      <th>610</th>\n      <td>[_START_, eminem, secret, gig, venue, revealed...</td>\n      <td>[_START_, fourth, album, rap, star, sale, two,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Build vocabularies - each word has an index, note : words sorted in ascending order\nall_tokens = train_df['source_text'].tolist() + train_df['summary_text'].tolist() + test_df['source_text'].tolist() + test_df['summary_text'].tolist()\nstoi = {actual_word: idx for idx, (word_num, actual_word) in enumerate(sorted(enumerate(set(token for tokens in all_tokens for token in tokens)), key=lambda x: x[1]))}\nitos = {idx: actual_word for idx, (word_num, actual_word) in enumerate(sorted(enumerate(set(token for tokens in all_tokens for token in tokens)), key=lambda x: x[1]))}","metadata":{"execution":{"iopub.status.busy":"2024-02-09T09:58:51.120739Z","iopub.execute_input":"2024-02-09T09:58:51.121100Z","iopub.status.idle":"2024-02-09T09:58:51.602926Z","shell.execute_reply.started":"2024-02-09T09:58:51.121067Z","shell.execute_reply":"2024-02-09T09:58:51.601907Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Init Bert Tokenizer and Bert Model for Embeddings\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")","metadata":{"_uuid":"767fd3a3-5c9d-4b04-b8f5-adf91e57c1b1","_cell_guid":"900bc2ee-c27c-4b12-a9a0-0d3b973ae03a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:51.604459Z","iopub.execute_input":"2024-02-09T09:58:51.604754Z","iopub.status.idle":"2024-02-09T09:58:55.105000Z","shell.execute_reply.started":"2024-02-09T09:58:51.604728Z","shell.execute_reply":"2024-02-09T09:58:55.103850Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55a6ab1e45254880960af015bb66b2c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9963bdca904476885dafaf8004eb1b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b918a0c5623422bb070480092d469c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e5b81d0a5e54eac8aec8f2a54801b63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec18a047fedf40d8bbe8c5278ad00291"}},"metadata":{}}]},{"cell_type":"code","source":"sentence = \"Hello, today is a thursday, I hope you have a great day Sanju Kutti\"\nencodings = bert_tokenizer.encode_plus(bert_tokenizer.tokenize(sentence),max_length=512)\nsentence = torch.tensor(encodings[\"input_ids\"]).unsqueeze(0)\nprint(sentence.shape)\nwith torch.no_grad():\n    output = bert_model(sentence).last_hidden_state\nprint(output.shape)","metadata":{"_uuid":"46e55077-48b7-4467-8d13-e040eaa374e9","_cell_guid":"06becbab-ae16-4e3d-bee8-ebd93d3364b4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:55.106424Z","iopub.execute_input":"2024-02-09T09:58:55.106730Z","iopub.status.idle":"2024-02-09T09:58:55.672727Z","shell.execute_reply.started":"2024-02-09T09:58:55.106703Z","shell.execute_reply":"2024-02-09T09:58:55.671599Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 20])\ntorch.Size([1, 20, 768])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using\",device)","metadata":{"_uuid":"e93e303f-3d29-460d-a111-868ed90cc48b","_cell_guid":"8cf18b62-d506-4f7d-b6bf-3bd4ad72e696","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:55.674305Z","iopub.execute_input":"2024-02-09T09:58:55.674739Z","iopub.status.idle":"2024-02-09T09:58:55.680945Z","shell.execute_reply.started":"2024-02-09T09:58:55.674704Z","shell.execute_reply":"2024-02-09T09:58:55.679833Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Using cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"print(bert_model.config.vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T09:58:55.682162Z","iopub.execute_input":"2024-02-09T09:58:55.682450Z","iopub.status.idle":"2024-02-09T09:58:55.694045Z","shell.execute_reply.started":"2024-02-09T09:58:55.682426Z","shell.execute_reply":"2024-02-09T09:58:55.692908Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"30522\n","output_type":"stream"}]},{"cell_type":"code","source":"print(list(stoi.items())[500:505])\nprint(list(itos.items())[500:505])","metadata":{"execution":{"iopub.status.busy":"2024-02-09T09:58:55.695331Z","iopub.execute_input":"2024-02-09T09:58:55.695639Z","iopub.status.idle":"2024-02-09T09:58:55.716849Z","shell.execute_reply.started":"2024-02-09T09:58:55.695609Z","shell.execute_reply":"2024-02-09T09:58:55.715810Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[('aggregator', 500), ('aggregators', 501), ('aggression', 502), ('aggressive', 503), ('aggressively', 504)]\n[(500, 'aggregator'), (501, 'aggregators'), (502, 'aggression'), (503, 'aggressive'), (504, 'aggressively')]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define a custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, source_texts, target_summaries, bert_tokenizer):\n        self.source_texts = source_texts\n        self.target_summaries = target_summaries\n        self.bert_tokenizer = bert_tokenizer\n\n    def __len__(self):\n        return len(self.source_texts)\n\n    def __getitem__(self, idx):\n        source_text = ' '.join([word for word in self.source_texts[idx]])\n        target_summary = ' '.join([word for word in self.target_summaries[idx]])\n        with torch.no_grad():\n            source_text = self.bert_tokenizer.encode_plus(self.bert_tokenizer.tokenize(source_text),max_length=512)\n            target_summary = self.bert_tokenizer.encode_plus(self.bert_tokenizer.tokenize(target_summary),max_length=512)\n        return torch.tensor(source_text['input_ids']), torch.tensor(target_summary['input_ids'])","metadata":{"_uuid":"6d7d5a8f-a1ad-4c2f-a21f-3e82823667ed","_cell_guid":"46bd33d9-801b-46ed-96ec-2615e7c6fadf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:55.718052Z","iopub.execute_input":"2024-02-09T09:58:55.718442Z","iopub.status.idle":"2024-02-09T09:58:55.729676Z","shell.execute_reply.started":"2024-02-09T09:58:55.718393Z","shell.execute_reply":"2024-02-09T09:58:55.729005Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Create custom datasets\ntrain_dataset = CustomDataset(train_df['source_text'].tolist(), train_df['summary_text'].tolist(),bert_tokenizer)\ntest_dataset = CustomDataset(test_df['source_text'].tolist(), test_df['summary_text'].tolist(),bert_tokenizer)","metadata":{"_uuid":"d9b69a91-2f5e-4d36-a732-b6973369af63","_cell_guid":"a54a744c-1146-4fbf-906e-bc6ca22c2e2e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:55.730820Z","iopub.execute_input":"2024-02-09T09:58:55.731129Z","iopub.status.idle":"2024-02-09T09:58:55.739750Z","shell.execute_reply.started":"2024-02-09T09:58:55.731105Z","shell.execute_reply":"2024-02-09T09:58:55.738903Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"'''\nNote : \nIn PyTorch, the `collate_fn` parameter in the `DataLoader` can be either a function or an object of a class. Both approaches are valid, and the choice depends on your preference and the complexity of your collation logic.\n\n1. Function as `collate_fn`:\ndef my_collate_fn(batch):\n    # Your custom collation logic here\n    return processed_batch\n# Use the function with DataLoader\ntrain_loader = DataLoader(dataset, batch_size=64, collate_fn=my_collate_fn)\n\n2. Class as `collate_fn`:\nclass MyCollateClass:\n    def __call__(self, batch):\n        # Your custom collation logic here\n        return processed_batch\n# Instantiate the class and use it with DataLoader\nmy_collate_instance = MyCollateClass()\ntrain_loader = DataLoader(dataset, batch_size=64, collate_fn=my_collate_instance)\n\nUsing a class allows you to maintain state between batches if needed, as the class instance retains its state between calls. This can be beneficial if your collation logic requires some persistent information.\n\nThe key point is that the `collate_fn` parameter should be a callable (a function or an object with a `__call__` method) that takes a list of batch data and returns the processed batch. The processing typically involves padding sequences, converting data types, or any other necessary steps to prepare the batch for the model.\n'''","metadata":{"_uuid":"18e145a3-ccf4-4682-8d42-5f9d454c9d7d","_cell_guid":"08a4bb2a-c30d-4ce9-8b18-c6fad1ece10e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:55.740764Z","iopub.execute_input":"2024-02-09T09:58:55.741019Z","iopub.status.idle":"2024-02-09T09:58:55.753150Z","shell.execute_reply.started":"2024-02-09T09:58:55.740998Z","shell.execute_reply":"2024-02-09T09:58:55.752245Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'\\nNote : \\nIn PyTorch, the `collate_fn` parameter in the `DataLoader` can be either a function or an object of a class. Both approaches are valid, and the choice depends on your preference and the complexity of your collation logic.\\n\\n1. Function as `collate_fn`:\\ndef my_collate_fn(batch):\\n    # Your custom collation logic here\\n    return processed_batch\\n# Use the function with DataLoader\\ntrain_loader = DataLoader(dataset, batch_size=64, collate_fn=my_collate_fn)\\n\\n2. Class as `collate_fn`:\\nclass MyCollateClass:\\n    def __call__(self, batch):\\n        # Your custom collation logic here\\n        return processed_batch\\n# Instantiate the class and use it with DataLoader\\nmy_collate_instance = MyCollateClass()\\ntrain_loader = DataLoader(dataset, batch_size=64, collate_fn=my_collate_instance)\\n\\nUsing a class allows you to maintain state between batches if needed, as the class instance retains its state between calls. This can be beneficial if your collation logic requires some persistent information.\\n\\nThe key point is that the `collate_fn` parameter should be a callable (a function or an object with a `__call__` method) that takes a list of batch data and returns the processed batch. The processing typically involves padding sequences, converting data types, or any other necessary steps to prepare the batch for the model.\\n'"},"metadata":{}}]},{"cell_type":"code","source":"# Define collate function for DataLoader\ndef collate_fn(batch):\n    sources, targets = zip(*batch)\n    padded_sources = pad_sequence(sources, batch_first=True)\n    padded_targets = pad_sequence(targets, batch_first=True)\n    return padded_sources, padded_targets","metadata":{"_uuid":"2d5d8275-0ac4-49be-b58c-653b0b3bafb5","_cell_guid":"043364db-8a6c-4b30-9690-870387393ff1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:55.754262Z","iopub.execute_input":"2024-02-09T09:58:55.754550Z","iopub.status.idle":"2024-02-09T09:58:55.766948Z","shell.execute_reply.started":"2024-02-09T09:58:55.754522Z","shell.execute_reply":"2024-02-09T09:58:55.766250Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Define the Encoder Architecture using LSTM\nclass Encoder(nn.Module):\n    def __init__(self, bert_model, embedding_dim, hidden_dim, n_layers, dropout):\n        super(Encoder, self).__init__()\n        self.bert_model = bert_model\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=True, dropout=dropout, batch_first=True)\n\n    def forward(self, X):\n        # X shape = [Batch_Size X Sequence_Len X 1]\n        assert X.shape[1] <= 512 # max size sequences that BERT can handle\n        with torch.no_grad():\n            X = self.bert_model(X).last_hidden_state\n        # X shape = [Batch_Size X Sequence_Len X embedding_dim]\n        X,(hidden_state,cell_state) = self.lstm(X)\n        # X shape = [Batch_Size X Seq_Len X Hidden_Dim] , Hidden_State_Shape = Cell_State_Shape = [Num_Layers X Batch_Size X Hidden_Dim]\n        return hidden_state,cell_state\n\n# Define the Decoder Architecture using LSTM\nclass Decoder(nn.Module):\n    def __init__(self, bert_model, target_vocab_size, embedding_dim, hidden_dim, n_layers, dropout):\n        super(Decoder, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.target_vocab_size = target_vocab_size\n        self.bert_model = bert_model\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, bidirectional=True, dropout=dropout, batch_first=True)\n        self.fc = nn.Linear(hidden_dim*2,target_vocab_size) # bidrectional hence \n\n    def forward(self, hidden_state, cell_state, Y, force_correction=0.5):\n        # Hidden_State_Shape = Cell_State_Shape = [Num_Layers X Batch_Size X Hidden_Dim]\n        # Y Shape = [Batch_Size X Sequence_Len]\n        \n        batch_size,seq_len = Y.shape[0],Y.shape[1]\n        outputs = torch.zeros(seq_len,batch_size,self.target_vocab_size).to(device) # [Batch_Size X Sequence_Len]\n        \n        X = Y[:,1]\n        # X shape = [Batch_Size X 1]\n        for i in range(seq_len):\n            X = X.unsqueeze(1) \n            # X shape = [Batch_Size X 1 X 1]\n            with torch.no_grad():\n                decoder_input = self.bert_model(X).last_hidden_state\n            # decoder_input_shape = [Batch_Size X 1 X Embedding_Dim]\n            assert decoder_input.shape[0]>0 and decoder_input.shape[1]>0\n            decoder_output,(hidden_state,cell_state) = self.lstm(decoder_input,(hidden_state,cell_state))\n            # Decoder_Output_Shape = [Batch_Size X 1 X hidden_dim]\n            decoder_output = self.fc(decoder_output)\n            # Decoder_Output_Shape = [Batch_Size X 1 X target_vocab_size]\n            # Store output\n            outputs[i] = decoder_output.permute(1,0,2).squeeze(0)\n            _ , indexes = decoder_output.max(dim=2)\n            # indexes shape = [Batch_Size X 1]\n            indexes = indexes.squeeze(1)\n            # use indexes as next input or correct it\n            X = indexes if random.random() < 0.5 else Y[:,i]\n            # indexes shape = X shape = [Batch_Size]\n            \n        # Output Shape = [Seq_Len X Batch_Size X Target_Vocab_Size]\n        outputs = outputs.permute(1,0,2)\n        outputs = outputs.reshape(-1,self.target_vocab_size)\n        # Output Shape = [Batch_Size X Seq_Len X Target_Vocab_Size]\n        return outputs","metadata":{"_uuid":"e701a107-dd0f-4b26-b071-56477bdea3e9","_cell_guid":"d4b8085d-1cf1-41bd-ae8a-43883decf3f3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:55.768283Z","iopub.execute_input":"2024-02-09T09:58:55.768614Z","iopub.status.idle":"2024-02-09T09:58:55.783825Z","shell.execute_reply.started":"2024-02-09T09:58:55.768583Z","shell.execute_reply":"2024-02-09T09:58:55.783014Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class EncDecLSTM(nn.Module):\n    def __init__(self,enc,dec):\n        super(EncDecLSTM,self).__init__()\n        self.enc = enc\n        self.dec = dec\n        \n    def forward(self,X,Y):\n        hidden_state,cell_state = self.enc(X)\n        output = self.dec(hidden_state,cell_state,Y)\n        return output","metadata":{"_uuid":"9a74bf88-9d6c-4363-8949-6a01c2b0b2b5","_cell_guid":"53bef3e6-c8f6-4a26-b600-777e487e613a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:55.784980Z","iopub.execute_input":"2024-02-09T09:58:55.785307Z","iopub.status.idle":"2024-02-09T09:58:55.798653Z","shell.execute_reply.started":"2024-02-09T09:58:55.785282Z","shell.execute_reply":"2024-02-09T09:58:55.797901Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Instantiate the model\noutput_dim = bert_model.config.vocab_size\nlearning_rate = 0.001\nembedding_dim = 768  \nhidden_dim = 512\nn_layers = 2\ndropout = 0.2\nnum_epochs = 20\nnum_workers = 3\n\nencoder = Encoder(bert_model, embedding_dim, hidden_dim, n_layers, dropout)\ndecoder = Decoder(bert_model, output_dim, embedding_dim, hidden_dim, n_layers, dropout)\nmodel = EncDecLSTM(encoder,decoder)\nprint(model)","metadata":{"_uuid":"7f7c4295-2263-4b6d-bc6b-f3292eca5f59","_cell_guid":"277eb600-7ff8-4537-9523-f6a0e8d7ac41","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:55.799691Z","iopub.execute_input":"2024-02-09T09:58:55.800371Z","iopub.status.idle":"2024-02-09T09:58:56.269925Z","shell.execute_reply.started":"2024-02-09T09:58:55.800339Z","shell.execute_reply":"2024-02-09T09:58:56.269031Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"EncDecLSTM(\n  (enc): Encoder(\n    (bert_model): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (lstm): LSTM(768, 512, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n  )\n  (dec): Decoder(\n    (bert_model): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (lstm): LSTM(768, 512, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n    (fc): Linear(in_features=1024, out_features=30522, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"trainable_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\nprint(trainable_params)","metadata":{"_uuid":"4ab353e1-b6ca-4416-9e0e-e21fc0f4d042","_cell_guid":"08f422e5-2622-4009-a9af-046d3e96f33d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:56.271234Z","iopub.execute_input":"2024-02-09T09:58:56.271615Z","iopub.status.idle":"2024-02-09T09:58:56.278541Z","shell.execute_reply.started":"2024-02-09T09:58:56.271580Z","shell.execute_reply":"2024-02-09T09:58:56.277613Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"121032960\n","output_type":"stream"}]},{"cell_type":"code","source":"trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(trainable_params)","metadata":{"_uuid":"61cd290d-00e4-4bc7-849c-547a025553fc","_cell_guid":"35ea08c6-d8d5-4da4-b347-6d5085b93464","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:56.284983Z","iopub.execute_input":"2024-02-09T09:58:56.285256Z","iopub.status.idle":"2024-02-09T09:58:56.291331Z","shell.execute_reply.started":"2024-02-09T09:58:56.285233Z","shell.execute_reply":"2024-02-09T09:58:56.290442Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"163868730\n","output_type":"stream"}]},{"cell_type":"code","source":"# Specify optimizer and loss function\noptimizer = optim.Adam(model.parameters(),lr=learning_rate)\nloss_fun = nn.CrossEntropyLoss()","metadata":{"_uuid":"e96fa9e8-c5be-4cc1-931d-57218cc5cc5a","_cell_guid":"8b1eb0a0-a1ec-4c88-909a-b9f03447e16f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:56.292406Z","iopub.execute_input":"2024-02-09T09:58:56.292691Z","iopub.status.idle":"2024-02-09T09:58:56.308436Z","shell.execute_reply.started":"2024-02-09T09:58:56.292668Z","shell.execute_reply":"2024-02-09T09:58:56.307603Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn, num_workers=num_workers)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)","metadata":{"_uuid":"feaf3b05-e23a-4335-a2e4-5747cfca4665","_cell_guid":"d2074be6-23b0-42d2-9a57-45d9f58f72fd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:56.309788Z","iopub.execute_input":"2024-02-09T09:58:56.310080Z","iopub.status.idle":"2024-02-09T09:58:56.320102Z","shell.execute_reply.started":"2024-02-09T09:58:56.310056Z","shell.execute_reply":"2024-02-09T09:58:56.319226Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"source_dummy,target_dummy = next(iter(train_loader))","metadata":{"_uuid":"e6d7609a-1211-4982-9045-5eb2acafe357","_cell_guid":"95b67a6a-93f3-49f6-9c39-9bd2bab10daf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:56.321222Z","iopub.execute_input":"2024-02-09T09:58:56.321517Z","iopub.status.idle":"2024-02-09T09:58:56.769567Z","shell.execute_reply.started":"2024-02-09T09:58:56.321489Z","shell.execute_reply":"2024-02-09T09:58:56.768493Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"print(source_dummy.shape,target_dummy.shape)","metadata":{"_uuid":"997ef9b3-127e-4454-9b89-3a6f861bc687","_cell_guid":"0533af63-7e19-4979-8cdc-c2369106ddc6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:56.771031Z","iopub.execute_input":"2024-02-09T09:58:56.771381Z","iopub.status.idle":"2024-02-09T09:58:56.777512Z","shell.execute_reply.started":"2024-02-09T09:58:56.771335Z","shell.execute_reply":"2024-02-09T09:58:56.776575Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"torch.Size([8, 443]) torch.Size([8, 224])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(torch.min(target_dummy),torch.max(target_dummy))","metadata":{"execution":{"iopub.status.busy":"2024-02-09T09:58:56.778909Z","iopub.execute_input":"2024-02-09T09:58:56.779284Z","iopub.status.idle":"2024-02-09T09:58:56.792459Z","shell.execute_reply.started":"2024-02-09T09:58:56.779249Z","shell.execute_reply":"2024-02-09T09:58:56.791494Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"tensor(0) tensor(29577)\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = model(source_dummy,target_dummy)\nprint(y_pred.shape,target_dummy.shape)","metadata":{"_uuid":"72e12e35-d2b8-4b26-bab0-bae78e8cb7ad","_cell_guid":"8a03d1e5-dca4-485b-9240-c4195be82280","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:58:56.793523Z","iopub.execute_input":"2024-02-09T09:58:56.793802Z","iopub.status.idle":"2024-02-09T09:59:22.705975Z","shell.execute_reply.started":"2024-02-09T09:58:56.793779Z","shell.execute_reply":"2024-02-09T09:59:22.705110Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1792, 30522]) torch.Size([8, 224])\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_loop(model,dataloader,loss_fun,optimizer,device):\n    model.train()\n    model.to(device)\n    min_loss = None\n    for epoch in range(num_epochs):\n        losses = []\n        accuracies = []\n        loop = tqdm(enumerate(dataloader), total=len(dataloader), leave=True)\n        for batch,(x,y) in loop:\n            # put on cuda\n            x = x.to(device)\n            y = y.to(device)\n    \n            # forward pass\n            y_pred = model(x,y)\n            \n            # calculate loss & accuracy\n            loss = loss_fun(y_pred,y.reshape(-1))\n            losses.append(loss.detach().item())\n            \n            accuracy = check_accuracy(y_pred,y.reshape(-1))\n            accuracies.append(accuracy.detach().item())\n            \n            # zero out prior gradients\n            optimizer.zero_grad()\n            \n            # backprop\n            loss.backward()\n            \n            # update weights\n            optimizer.step()\n            \n            # Update TQDM progress bar\n            loop.set_description(f\"Epoch [{epoch}/{num_epochs}] \")\n            loop.set_postfix(loss=loss.detach().item(), accuracy=accuracy.detach().item())\n\n        moving_loss = sum(losses) / len(losses)\n        moving_accuracy = sum(accuracies) / len(accuracies)\n        checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n        # Save check point\n        if min_loss == None:\n            min_loss = moving_loss\n            save_checkpoint(checkpoint)\n        elif moving_loss < min_loss:\n            min_loss = moving_loss\n            save_checkpoint(checkpoint)\n        print('Epoch {0} : Loss = {1} , Training Accuracy={2}'.format(epoch, moving_loss, moving_accuracy))","metadata":{"_uuid":"f1e3fed7-78bd-4512-b670-7542eab6abad","_cell_guid":"7400fc0b-dbe2-4935-9e5a-dcab47e23275","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:59:22.707025Z","iopub.execute_input":"2024-02-09T09:59:22.707290Z","iopub.status.idle":"2024-02-09T09:59:22.719083Z","shell.execute_reply.started":"2024-02-09T09:59:22.707268Z","shell.execute_reply":"2024-02-09T09:59:22.718256Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_loop(model,train_loader,loss_fun,optimizer,device)","metadata":{"_uuid":"3d0cdde5-7cf9-4b33-befc-62b2f9da463b","_cell_guid":"d133f546-0ae6-416c-8b66-9527fe6bc73b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T09:59:22.720414Z","iopub.execute_input":"2024-02-09T09:59:22.720787Z","iopub.status.idle":"2024-02-09T16:07:16.321145Z","shell.execute_reply.started":"2024-02-09T09:59:22.720751Z","shell.execute_reply":"2024-02-09T16:07:16.319670Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"Epoch [0/20] : 100%|| 223/223 [18:41<00:00,  5.03s/it, accuracy=24.5, loss=6.66]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 0 : Loss = 4.989009191102511 , Training Accuracy=44.063086676490684\n","output_type":"stream"},{"name":"stderr","text":"Epoch [1/20] : 100%|| 223/223 [18:26<00:00,  4.96s/it, accuracy=37.4, loss=5.45]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 1 : Loss = 4.746741835846494 , Training Accuracy=44.27731052107875\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/20] : 100%|| 223/223 [18:16<00:00,  4.92s/it, accuracy=58.5, loss=3.4] \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 2 : Loss = 4.644630534766501 , Training Accuracy=45.06716356149169\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/20] : 100%|| 223/223 [18:02<00:00,  4.85s/it, accuracy=44.7, loss=4.63]","output_type":"stream"},{"name":"stdout","text":"Epoch 3 : Loss = 4.663947915817055 , Training Accuracy=44.71359029692919\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch [4/20] : 100%|| 223/223 [18:23<00:00,  4.95s/it, accuracy=47.6, loss=4.44]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 4 : Loss = 4.572192267986691 , Training Accuracy=45.53478465914192\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/20] : 100%|| 223/223 [18:24<00:00,  4.95s/it, accuracy=24.6, loss=6.5] \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 5 : Loss = 4.551498611946277 , Training Accuracy=45.45389306277972\n","output_type":"stream"},{"name":"stderr","text":"Epoch [6/20] : 100%|| 223/223 [18:19<00:00,  4.93s/it, accuracy=39.7, loss=4.87]","output_type":"stream"},{"name":"stdout","text":"Epoch 6 : Loss = 4.563818603353115 , Training Accuracy=44.84078839220808\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch [7/20] : 100%|| 223/223 [18:21<00:00,  4.94s/it, accuracy=39.5, loss=5.01]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 7 : Loss = 4.440578717287345 , Training Accuracy=45.858709070180026\n","output_type":"stream"},{"name":"stderr","text":"Epoch [8/20] : 100%|| 223/223 [18:16<00:00,  4.92s/it, accuracy=18.9, loss=6.63]","output_type":"stream"},{"name":"stdout","text":"Epoch 8 : Loss = 4.457912401233553 , Training Accuracy=45.1254554286666\n","output_type":"stream"},{"name":"stderr","text":"\nEpoch [9/20] : 100%|| 223/223 [18:32<00:00,  4.99s/it, accuracy=24.2, loss=6.32]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 9 : Loss = 4.384627342758692 , Training Accuracy=45.35863589812822\n","output_type":"stream"},{"name":"stderr","text":"Epoch [10/20] : 100%|| 223/223 [18:18<00:00,  4.93s/it, accuracy=17.3, loss=6.45]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 10 : Loss = 4.338240559325625 , Training Accuracy=45.13006490014594\n","output_type":"stream"},{"name":"stderr","text":"Epoch [11/20] : 100%|| 223/223 [18:13<00:00,  4.90s/it, accuracy=22.8, loss=6.27]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 11 : Loss = 4.308947736372327 , Training Accuracy=44.84712798285378\n","output_type":"stream"},{"name":"stderr","text":"Epoch [12/20] : 100%|| 223/223 [18:09<00:00,  4.89s/it, accuracy=31.4, loss=5.49]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 12 : Loss = 4.246689762235222 , Training Accuracy=45.129745098507456\n","output_type":"stream"},{"name":"stderr","text":"Epoch [13/20] : 100%|| 223/223 [18:26<00:00,  4.96s/it, accuracy=48.9, loss=3.93]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 13 : Loss = 4.189839440610911 , Training Accuracy=45.00220037896537\n","output_type":"stream"},{"name":"stderr","text":"Epoch [14/20] : 100%|| 223/223 [18:14<00:00,  4.91s/it, accuracy=25.8, loss=5.43]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 14 : Loss = 4.1783909284480485 , Training Accuracy=44.40167400548276\n","output_type":"stream"},{"name":"stderr","text":"Epoch [15/20] : 100%|| 223/223 [18:11<00:00,  4.90s/it, accuracy=30.9, loss=5.21]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 15 : Loss = 4.09980836149823 , Training Accuracy=44.66537544866314\n","output_type":"stream"},{"name":"stderr","text":"Epoch [16/20] : 100%|| 223/223 [18:27<00:00,  4.97s/it, accuracy=23.5, loss=5.69]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 16 : Loss = 3.968940021211256 , Training Accuracy=45.52538209752652\n","output_type":"stream"},{"name":"stderr","text":"Epoch [17/20] : 100%|| 223/223 [18:26<00:00,  4.96s/it, accuracy=35.8, loss=4.57]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 17 : Loss = 3.9371425431940055 , Training Accuracy=45.18788345405339\n","output_type":"stream"},{"name":"stderr","text":"Epoch [18/20] : 100%|| 223/223 [18:38<00:00,  5.01s/it, accuracy=27.1, loss=5.8] \n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 18 : Loss = 3.8534800717648903 , Training Accuracy=45.54622865685433\n","output_type":"stream"},{"name":"stderr","text":"Epoch [19/20] : 100%|| 223/223 [18:18<00:00,  4.93s/it, accuracy=28.2, loss=4.97]\n","output_type":"stream"},{"name":"stdout","text":"Saving weights-->\nEpoch 19 : Loss = 3.833954502114266 , Training Accuracy=45.190733366482995\n","output_type":"stream"}]},{"cell_type":"code","source":"def test_loop(model,dataloader,loss_fun,device):\n    model.eval()\n    model.to(device)\n    losses = []\n    samples,correct = 0,0\n    loop = tqdm(enumerate(dataloader), total=len(dataloader), leave=True)\n    with torch.no_grad():\n        for batch,(x,y) in loop:\n            # put on cuda\n            x = x.to(device)\n            y = y.to(device)\n\n            # forward pass\n            y_pred = model(x,y)\n            \n            # caclulate test loss\n            loss = loss_fun(y_pred,y.reshape(-1))\n            losses.append(loss.item())\n\n            # accuracy over entire dataset\n            _,predpos=y_pred.max(1)\n            samples+=len(y.reshape(-1))\n            correct+=(predpos==y.reshape(-1)).sum().item()\n            \n            # Update TQDM progress bar\n            loop.set_postfix(loss=loss.item())\n\n    print(\"Final Test Accuracy = \",100 * (correct/samples))","metadata":{"_uuid":"aa0e9514-c139-42a4-baf2-c60e1a4ea2d4","_cell_guid":"79a91b46-bf45-436a-b90c-659ec99fc05a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T16:07:16.323840Z","iopub.execute_input":"2024-02-09T16:07:16.324176Z","iopub.status.idle":"2024-02-09T16:07:16.333476Z","shell.execute_reply.started":"2024-02-09T16:07:16.324139Z","shell.execute_reply":"2024-02-09T16:07:16.332653Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"test_loop(model,test_loader,loss_fun,device)","metadata":{"_uuid":"762d68a9-2cd0-47e9-a014-09d7e7c126a5","_cell_guid":"b1f7396c-b765-4e86-bf12-3255a83c84fe","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-02-09T16:07:16.334437Z","iopub.execute_input":"2024-02-09T16:07:16.334749Z","iopub.status.idle":"2024-02-09T16:10:19.917249Z","shell.execute_reply.started":"2024-02-09T16:07:16.334711Z","shell.execute_reply":"2024-02-09T16:10:19.916134Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"100%|| 56/56 [03:03<00:00,  3.28s/it, loss=5.18]","output_type":"stream"},{"name":"stdout","text":"Final Test Accuracy =  48.537226904289014\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}