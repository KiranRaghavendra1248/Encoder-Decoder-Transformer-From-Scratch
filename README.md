# Encoder Decoder Transformer from Scratch
<ul>
  <li> Implemented Encoder-Decoder transformer from scratch with multi-head attention mechanism and positional encoding from research paper for abstractive text summarization.
  <li> Created custom vocabulary and dataset in Pytorch to fetch text samples and their summaries for training and validation data loader
  <li> Compared context-independent GloVe word embeddings and context-dependent BERT word embeddings for abstractive text summarization using Encoder-Decoder Bi-LSTM with 2 layers, using cosine annealing with Adam optimizer. 
  <li> Evaluated and compared accuracy of Encoder-Decoder transformer architecture against Encoder-Decoder Bi-LSTM.
</ul>
